{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Representation\n",
    "Zimei Yang |\n",
    "May 13 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I BoW and TF-IDF\n",
    "1. Use Amazon book reviews (text documents) dataset.\n",
    "2. Use Bag of Words (BoW) and TF-IDF (CountVectorizer and TfidfVectorizer in scikit-learn).\n",
    "3. Write Python program to create and print vocabulary and document-term matrix (vectorized representation). \n",
    "4. Try unigram and bigram parameters and observe their effect on number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load Amazon book reviews dataset\n",
    "df = pd.read_csv(\"Small-Book Reviews from Amazon.csv\", names=['number','review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Ok~ but I think the Keirsey Temperment Test is...\n",
       "1    Repellent Sale of Conservativism  The fatalist...\n",
       "2    I had a bad feeling about this!  And I was rig...\n",
       "3    Lost Credability, QUICKLY!!  I admit, I haven'...\n",
       "4    Poorly written  I tried reading this book but ...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words Representation\n",
    "BoW representation is implemented in CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import CountVectorizer for BoW\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define corpus\n",
    "corpus = df.review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the CountVectorizer does the following: \n",
    "1. tokenizing \n",
    "2. building the vocabulary (*vocabulary is a mapping of terms to feature indices.*)\n",
    "  -- vocabulary can be accessed through the \"vocabulary_\" attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit countvectorizer to tokenize the corpus and build the corpus's vocabulary \n",
    "vect.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ok', 552), ('but', 116), ('think', 792), ('the', 782), ('keirsey', 436), ('temperment', 774), ('test', 777), ('is', 417), ('more', 508), ('accurate', 16), ('and', 47), ('cheaper', 135), ('this', 794), ('book', 108), ('has', 352), ('its', 425), ('good', 335), ('points', 593), ('if', 386), ('anything', 54), ('it', 424), ('helps', 361), ('you', 899), ('put', 623), ('into', 410), ('words', 884), ('what', 862), ('want', 847), ('from', 318), ('supervisor', 753)]\n"
     ]
    }
   ],
   "source": [
    "# see the head of our vocabulary (terms and indices) \n",
    "print(list(vect.vocabulary_.items())[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "904\n"
     ]
    }
   ],
   "source": [
    "# see length of the vocabulary\n",
    "print(len(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the BoW representation for the corpus, call the transform method, which creates a document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create BoW representation for the corpus\n",
    "bow = vect.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 904)\n"
     ]
    }
   ],
   "source": [
    "# see dimension of the BoW matrix\n",
    "print(bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 3 0 0]\n",
      " [0 1 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# see review vectors in BoW representation\n",
    "print(bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '1000', '14th', '18th', '1953', '1955', '1960', '1970', '60', 'abandon', 'able', 'about', 'academic', 'accomplished', 'according', 'account', 'accurate', 'act', 'actively', 'actually', 'ade', 'admit', 'adolescent', 'adulation', 'adventure', 'advocate', 'advocating', 'after', 'against', 'age']\n"
     ]
    }
   ],
   "source": [
    "# see feature names\n",
    "feature_names = vect.get_feature_names()\n",
    "print(feature_names[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "904\n"
     ]
    }
   ],
   "source": [
    "# see number of features\n",
    "number_of_features = len(feature_names)\n",
    "print(number_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Representation\n",
    "\n",
    "TF-IDF representation is implemented in TfidfVectorizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer(stop_words = 'english') #excluding stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the TfidfVectorizer does the following: \n",
    "1. tokenizing\n",
    "2. building the vocabulary\n",
    "3. applying tf-idf transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the TF-IDF Vectorizer to corpus\n",
    "vect.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ok', 462), ('think', 667), ('keirsey', 367), ('temperment', 658), ('test', 661), ('accurate', 15), ('cheaper', 101), ('book', 79), ('good', 284), ('points', 490), ('helps', 306), ('words', 734), ('want', 710), ('supervisor', 637), ('online', 464), ('does', 181), ('account', 14), ('difference', 169), ('options', 467), ('exactly', 215), ('like', 392), ('don', 187), ('messes', 422), ('results', 562), ('did', 167), ('just', 365), ('denial', 157), ('taken', 649), ('lot', 402), ('personality', 483)]\n"
     ]
    }
   ],
   "source": [
    "# see the head part of the vocabulary\n",
    "print(list(vect.vocabulary_.items())[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749\n"
     ]
    }
   ],
   "source": [
    "# see the total number of temrs in the vocabulary (excluding stop words)\n",
    "print(len(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the TF-IDF representation for the corpus, call the transform method, which creates a document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform corpus and create TF-IDF representation\n",
    "tfidf = vect.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 749)\n"
     ]
    }
   ],
   "source": [
    "# see dimension of the tfidf matrix\n",
    "print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09036659 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.03673022 0.03673022 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.11364664 0.         0.11364664]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.08401684 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# see reviews vector in TF-IDF representation\n",
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram and Bigram Parameters\n",
    "Use unigram and bigram parameters and observe their effect on number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x4890 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 5360 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use only unigram\n",
    "vect1 = CountVectorizer(ngram_range=(1, 1))\n",
    "vect1.fit_transform(corpus)\n",
    "# use only bigram \n",
    "vect2 = CountVectorizer(ngram_range=(2, 2))\n",
    "vect2.fit_transform(corpus)\n",
    "# use both unigram and bigram\n",
    "vect1_2 = CountVectorizer(ngram_range=(1, 2))\n",
    "vect1_2.fit_transform(corpus)\n",
    "# use unigram, bigram and trigram \n",
    "vect1_3 = CountVectorizer(ngram_range=(1, 3))\n",
    "vect1_3.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use only unigram: 904\n",
      "use only bigram: 1916\n",
      "use both unigram and bigram: 2820\n",
      "use unigram, bigram and trigram: 4890\n"
     ]
    }
   ],
   "source": [
    "# see number of features of using different ngram parameters\n",
    "print(\"use only unigram:\",len(vect1.get_feature_names()))\n",
    "print(\"use only bigram:\",len(vect2.get_feature_names()))\n",
    "print(\"use both unigram and bigram:\",len(vect1_2.get_feature_names()))\n",
    "print(\"use unigram, bigram and trigram:\",len(vect1_3.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II Speech-to-Text Cognitive Services Features\n",
    "Explore speech-to-text services from the four vendors (Microsoft, Amazon, Google, IBM) and list key  features of each service. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Microsoft Azure Speech-to-Text API\n",
    "1. Create custom language models tailored to users’ speaking styles\n",
    "   -- Customize the language model of your app’s speech recognition by tailoring it to your industry expressions, technical, geography or market terms, and even speaker style.\n",
    "   \n",
    "2. Adapt to user environment with custom acoustic models\n",
    "   -- Make sure your app’s speech recognition can function in all environments, account for background noise and match your users’ expected environments.\n",
    "   \n",
    "3. Use robust speech models from Microsoft\n",
    "  -- Enable powerful, personalized speech recognition by building your own customized speech recognition models on top of Microsoft’s existing state-of-the-art models.\n",
    "  \n",
    "   Retrieved From: https://azure.microsoft.com/en-us/services/cognitive-services/speech-to-text/\n",
    "\n",
    "\n",
    "------\n",
    "#### AWS Amazon Transcribe\n",
    "1. Simple-to-Use API\n",
    "   -- No complicated programming is required. Just call the API with a few lines of code, and Amazon Transcribe will return the text from your audio file stored in Amazon S3.\n",
    "2. Support for a Wide Range of Use Cases and Audio Quality\n",
    "   -- Provide accurate and automated trancripts for a wide range of audio quality. You can generate subtitles for any video or audio files, and even transcribe low quality telephony recordings such as customer service calls.\n",
    "3. Easy-to-Read Transcriptions with Punctuation\n",
    "   -- Most speech recognition systems output a string of text without punctuation. Amazon Transcribe uses deep learning to add punctuation and formatting automatically, so that the output is more intelligible and can be used without any further editing.\n",
    "4. Custom Vocabulary\n",
    "   -- Give you the ability to expand and customize the speech recognition vocabulary. You can add new words to the base vocabulary and generate highly-accurate transcriptions specific to your use case, such as product names, domain-specific terminology, or names of individuals.\n",
    "5. Timestamp Generation\n",
    "   -- Return a timestamp for each word, so that you can easily locate the audio in the original recording by searching for the text.\n",
    "6. Recognize Multiple Speakers\n",
    "   -- Recognize even when the speaker changes and attribute the transcribed text appropriately. This can significantly reduce the amount of work needed to transcribe audio with multiple speakers like telephone calls, meetings, and television shows.\n",
    "\n",
    "   Retrieved From: https://aws.amazon.com/transcribe/\n",
    "\n",
    "\n",
    "\n",
    "------\n",
    "#### Google Cloud Speech-toText | Speech Recognition\n",
    "1. Automatic Speech Recognition\n",
    "   -- Powered by deep learning neural networking to power your applications like voice search or speech transcription.\n",
    "2. Global Vocabulary in 120 Languages!\n",
    "   -- Recognizes 120 languages and variants with an extensive vocabulary.\n",
    "3. Word Hints\n",
    "   -- Can be customized to a specific context by providing a set of words and phrases that are likely to be spoken. Especially useful for adding custom words and names to the vocabulary and in voice-control use cases.\n",
    "4. Real-time Streaming or Pre-recorded Audio Support\n",
    "   -- Audio input can be streamed from by an application’s microphone or sent from a pre-recorded audio file (inline or through Google Cloud Storage). Multiple audio encodings are supported, including FLAC, AMR, PCMU and Linear-16.\n",
    "5. Noise Robustness\n",
    "   -- Handles noisy audio from many environments without requiring additional noise cancellation.\n",
    "6. Inappropriate Content Filtering\n",
    "   -- Filter inappropriate content in text results for some languages.\n",
    "7. Automatic Punctuation\n",
    "   -- Accurately punctuates transcriptions (i.e. commas, questions marks, and periods) with machine learning.\n",
    "8. Model Selection\n",
    "   -- Choose from a selection of four pre-built models: default, voice commands and search, phone calls, and video transcription.\n",
    "\n",
    "   Retrieved From: https://cloud.google.com/speech-to-text/\n",
    "\n",
    "\n",
    "----\n",
    "#### IBM Watson Speech-to-Text\n",
    "1. Powerful real-time speech recognition\n",
    "   -- Automatically transcribe audio from 7 languages in real-time. Rapidly identify and transcribe what is being discussed, even from lower quality audio, across a variety of audio formats and programming interfaces (HTTP REST, Websocket, Asynchronous HTTP)\n",
    "2. Highly accurate speech engine\n",
    "   -- Customize your model to improve accuracy for language and content you care most about, such as product names, sensitive subjects or names of individuals. Recognizes different speakers in your audio Spot specified keywords in real-time with high accuracy and confidence\n",
    "3. Built to support various use cases\n",
    "   -- Transcribe audio for various use cases ranging from real-time transcription for audio from a microphone, to analyzing 1000s of audio recording from your call center to provide meaningful analytics\n",
    "   \n",
    "   Retrieved From: https://www.ibm.com/watson/services/speech-to-text/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
